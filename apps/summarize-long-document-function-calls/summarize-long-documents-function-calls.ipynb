{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3af9e12",
   "metadata": {},
   "source": [
    "# How to make an AI that uses functions to summarize long documents\n",
    "\n",
    "This tutorial takes you through an example of using OpenAI's new function calling feature to download academic papers and summarize them based on user queries. Upon completion you will be prepared to take on more production-ready scenarios of deploying functions with your own knowledge bases and other internal services.\n",
    "\n",
    "### Getting started (WIP - this section to be rewritten)\n",
    "\n",
    "Some basic knowledge of Python and GitHub is helpful for this tutorial. Before diving in, make sure to set up an OpenAI API key and walk through the quickstart tutorial. This will give a good intuition on how to use the API to its full potential.\n",
    "\n",
    "Python is used as the main programming language along with the OpenAI, Pandas, transformers, NumPy, and other popular packages. If you run into any issues working through this tutorial, please ask a question on the OpenAI Community Forum.\n",
    "\n",
    "To start with the code, clone the full code for this tutorial on GitHub. Alternatively, follow along and copy each section into a Jupyter notebook and run the code step by step, or just read along. A good way to avoid any issues is to set up a new virtual environment and install the required packages by running the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc971ec8",
   "metadata": {},
   "source": [
    "## Wiring up the search service\n",
    "\n",
    "The main focus of this tutorial is building a function to work with a search service, so if the search service itself isn't of interest then please skip down to the next section on **Defining a function**. This section takes you through the process of setting up a search service to download papers and summarize them to answer user questions.\n",
    "\n",
    "For this example we'll use the capable search service from [arXiv](https://arxiv.org/search/), which stores a huge quantity of academic papers across a range of scientific disciplines. This can be replaced by any knowledge base, document repository or other repository of text that is relevant for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f91f279",
   "metadata": {},
   "source": [
    "### Search and download articles\n",
    "\n",
    "The first step is to create a directory to hold the papers that we download based on user searches, and to create an empty ```csv``` file that will hold references to our downloaded files so we can summarize the most relevant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2f491363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"papers\")\n",
    "paper_dir_filepath = \"arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01adff9e",
   "metadata": {},
   "source": [
    "We'll also define what models to use for this task. Our ```GPT_MODEL``` will be ```gpt-3.5-turbo-0613``` as this has been enabled for the function calling feature - more details on compatible models can be found in our function calling [documentation](https://platform.openai.com/docs/guides/gpt/function-calling). We've also gone with ```text-embedding-ada-002``` because this is currently our only widely used embeddings model, though more may come in future and will be found in our [models](https://platform.openai.com/docs/models/embeddings) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371424d",
   "metadata": {},
   "source": [
    "This function takes in text and returns an embedding that can be used for a similarity search to find the most relevant search result to the user's query. To learn more about embeddings please refer to our [documentation](https://platform.openai.com/docs/guides/embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103c65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f04df4",
   "metadata": {},
   "source": [
    "The ```get_articles``` function uses the ```arxiv``` Python library to take in a user query and return the most relevant search results. For each retrieved paper, we:\n",
    "- Create an empty dictionary\n",
    "- Fill it with the title, summary, article and download URLs\n",
    "- Download a PDF and save it to our ```papers``` directory\n",
    "- Create an embedding of the title and summary together for later retrieval\n",
    "- Store the title, summary, filepath and embedding in our library file ```arxiv_library.csv``` to be retrieved against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "38216163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from csv import writer\n",
    "\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=3):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references for library file\n",
    "        text_for_embedding = f\"Title: {result.title}\\nSummary: {result.summary}\"\n",
    "        response = embedding_request(text=text_for_embedding)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.summary,\n",
    "            result.download_pdf(data_dir),\n",
    "            response[\"data\"][0][\"embedding\"],\n",
    "        ]\n",
    "\n",
    "        # Write file_reference to library file\n",
    "        print(f'Downloading \"{result.title}\" to knowledge base')\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef17a92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading \"Large Language Model Guided Tree-of-Thought\" to knowledge base\n",
      "Downloading \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\" to knowledge base\n",
      "Downloading \"Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models\" to knowledge base\n",
      "Downloading \"Reasoning with Language Model is Planning with World Model\" to knowledge base\n",
      "Downloading \"Categorical Buechi and Parity Conditions via Alternating Fixed Points of Functors\" to knowledge base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Large Language Model Guided Tree-of-Thought',\n",
       " 'summary': \"In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel\\napproach aimed at improving the problem-solving capabilities of auto-regressive\\nlarge language models (LLMs). The ToT technique is inspired by the human mind's\\napproach for solving complex reasoning tasks through trial and error. In this\\nprocess, the human mind explores the solution space through a tree-like thought\\nprocess, allowing for backtracking when necessary. To implement ToT as a\\nsoftware system, we augment an LLM with additional modules including a prompter\\nagent, a checker module, a memory module, and a ToT controller. In order to\\nsolve a given problem, these modules engage in a multi-round conversation with\\nthe LLM. The memory module records the conversation and state history of the\\nproblem solving process, which allows the system to backtrack to the previous\\nsteps of the thought-process and explore other directions from there. To verify\\nthe effectiveness of the proposed technique, we implemented a ToT-based solver\\nfor the Sudoku Puzzle. Experimental results show that the ToT framework can\\nsignificantly increase the success rate of Sudoku puzzle solving. Our\\nimplementation of the ToT-based Sudoku solver is available on GitHub:\\n\\\\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.\",\n",
       " 'article_url': 'http://arxiv.org/abs/2305.08291v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2305.08291v1'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_output = get_articles('Tree of thought reasoning')\n",
    "result_output[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08cb992",
   "metadata": {},
   "source": [
    "### Summarize the most relevant article\n",
    "\n",
    "This next block of functions takes in a filepath to the most relevant downloaded PDF, and does the following to break long documents into manageable pieces:\n",
    "- ```read_pdf```: Reads in the PDF text and page numbers, and returns it as one long string.\n",
    "- ```create_chunks```: Intelligently breaks the PDF into n-length token chunks (in our demonstration we use 1500), avoiding stopping in the middle of sentences.\n",
    "- ```extract_chunk```: Applies a prompt to a chunk of text using OpenAI's ```ChatCompletion``` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6314c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarized chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1beae3",
   "metadata": {},
   "source": [
    "This next block contains the ```summarize_text``` function, which will be the key one driving our user results. In it we:\n",
    "- Read in our library file and get the filepath for the most relevant paper to the user's query using the ```search_embeddings``` function.\n",
    "- Read in the PDF using ```read_pdf``` and chunk it up into 1500 tokens using ```create_chunks```.\n",
    "- Summarize every chunk using the ```summary_prompt``` and the ```extract_chunk``` function. Because papers can have many chunks, we do this in parallel using ```concurrent```.\n",
    "- Perform a final summary of the summaries into a standard format which should answer the user's question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f59378ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import concurrent\n",
    "from openai.embeddings_utils import distances_from_embeddings\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def search_embeddings(query,df,top_n=1):\n",
    "    \n",
    "    query_embedding = embedding_request(query)\n",
    "    \n",
    "    df['distances'] = distances_from_embeddings(query_embedding[\"data\"][0][\"embedding\"]\n",
    "                                              , df['embedding'].values\n",
    "                                              , distance_metric='cosine')\n",
    "    \n",
    "    return list(df.sort_values('distances', ascending=True)['filepath'])[:top_n]\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"summary\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = search_embeddings(query, library_df,top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9711a407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n",
      "Core Argument:\n",
      "The Reasoning via Planning (RAP) framework combines world models, rewards, and Monte Carlo Tree Search (MCTS) planning to enable large language models (LLMs) to perform complex reasoning tasks. RAP addresses the limitations of LLMs by incorporating a principled planning algorithm and utilizing the LLM as both a world model and a reasoning agent.\n",
      "\n",
      "Evidence:\n",
      "- RAP repurposes LLMs as world models and reasoning agents, allowing for grounded and coherent inference.\n",
      "- Rewards are designed to assess the feasibility and desirability of reasoning steps.\n",
      "- MCTS planning algorithm is used to explore the reasoning space and find optimal reasoning traces.\n",
      "- RAP outperforms strong baselines in tasks such as plan generation, math reasoning, and logical inference.\n",
      "\n",
      "Conclusions:\n",
      "- RAP demonstrates superiority in plan generation, achieving a 33% relative improvement compared to the Chain-of-Thought baseline.\n",
      "- RAP achieves high accuracy in math reasoning and logical reasoning tasks, outperforming baselines in various iterations/samples.\n",
      "- RAP has the potential to redefine LLM reasoning and achieve human-level strategic thinking and planning in artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n",
    "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbceefa9",
   "metadata": {},
   "source": [
    "## Defining a function\n",
    "\n",
    "At this stage we have a working search service that reads articles from arXiv and summarizes them to answer a user question. We now need to wrap this in a ```function``` definition so that ```ChatCompletion``` can take it in and make it available to our LLM.\n",
    "\n",
    "An OpenAI ```function``` is defined with a ```name``` that the model will identify it with, a ```description``` that describes when to use it, and ```parameters``` that define what information the LLM needs to gather to use the function. The ```parameters``` can be required or optional, and you can also force the LLM to call a function (or not to call one) should your application logic demand it - for more details on these please refer to [this cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb) and our function calling [docs](https://platform.openai.com/docs/guides/gpt/function-calling).\n",
    "\n",
    "The LLM should then check the user's message each time to see whether a ```function``` is necessary, and if it does it will return a response that includes the ```finish_reason``` of ```function_call```, triggering your application to call the named function. \n",
    "\n",
    "For our example, we'll define a function called ```read_article_and_summarize```. This will extract a ```query```, which is simply what the user searched for, and a ```previous_search``` enum to identify whether the user has searched this topic before. If they haven't, our application will trigger ```get_articles``` first so we have some relevant articles to answer their question with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "daf53f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to answer the user's question using arXiv papers.\n",
    "        You should use this tool to answer all questions that you haven't already answered.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User's query in plain text.\n",
    "                            \"\"\",\n",
    "                },\n",
    "                \"previous_search\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"yes\",\"no\"],\n",
    "                    \"description\": \"\"\"If the customer has asked a previous question on this topic say yes, otherwise say no.\n",
    "                                    Infer this from the conversation history if the customer has asked a similar question\"\"\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\",\"previous_search\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d48428a",
   "metadata": {},
   "source": [
    "## Building a question & answer agent to use the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf43d24",
   "metadata": {},
   "source": [
    "This wrapper function makes a ```ChatCompletion``` request that optionally provides ```functions``` as well. If the ```finish_reason``` is ```function_call```, then we execute the right functions to get the user the answer they need.\n",
    "\n",
    "**This is an important concept** when working with ```functions```. OpenAI provides you the name of the function to call and the arguments that you should supply, but we do not call the function for you - you must write the logic to do this yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4c709241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(model=GPT_MODEL\n",
    "                                            ,messages=messages\n",
    "                                            ,functions=functions)\n",
    "    full_message = response[\"choices\"][0]\n",
    "        \n",
    "    if full_message[\"finish_reason\"] == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b222601c",
   "metadata": {},
   "source": [
    "The ```call_arxiv_function``` goes through the logic of figuring out what to do when the ```function_call``` is received. Here we have built in some logic so that if there hasn't been a ```previous_search``` on a similar topic for this user, we will execute a search first so that we have a relevant paper to summarize.\n",
    "\n",
    "Once we have a relevant paper, we summarize it using the ```summarize_text``` function and provide the response back to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2077e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\":\n",
    "        try:\n",
    "            previous_search = json.loads(\n",
    "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "            )[\"previous_search\"]\n",
    "\n",
    "        except:\n",
    "            previous_search = \"no\"\n",
    "\n",
    "        if previous_search == \"no\":\n",
    "            try:\n",
    "                parsed_output = json.loads(\n",
    "                    full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "                )\n",
    "                print(\"Getting search results\")\n",
    "                results = get_articles(parsed_output[\"query\"])\n",
    "\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"function\",\n",
    "                        \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
    "                        \"content\": str(results),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(parsed_output)\n",
    "                print(f\"Function execution failed\")\n",
    "                print(f\"Error message: {e}\")\n",
    "\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef10cf9",
   "metadata": {},
   "source": [
    "## Testing our agent\n",
    "\n",
    "We now have all the pieces needed to test our agent. We'll initiate a ```messages``` list to hold our interactions between the different roles in our conversation. The key ```roles``` used in the ```ChatCompletion``` API are:\n",
    "- ```system```: This is a guiding message which dictates the behaviour you want the LLM to display and any instructions you'd like to provide. In most UX's this is hidden from the user.\n",
    "- ```user```: This is what the user interacting with the LLM has input - in this example you can imagine a user has a search bar available that they are typing their queries into.\n",
    "- ```assistant```: Outputs from a call to ```ChatCompletion``` are always received with ```role``` = ```assistant```. This is the response from the LLM, which can be directed at either the ```user```, or at a ```function``` if the ```finish_reason``` is ```function_call```. \n",
    "- ```function```: The output from a function call being provided to the LLM to make its final response.\n",
    "\n",
    "For this example we'll build up three turns of conversation with the LLM, with each one using either a function or directly responding to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9a47cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Getting search results\n",
      "Downloading \"Large Language Model Guided Tree-of-Thought\" to knowledge base\n",
      "Downloading \"Reasoning with Language Model is Planning with World Model\" to knowledge base\n",
      "Downloading \"Temporal Data Meets LLM -- Explainable Financial Time Series Forecasting\" to knowledge base\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 16/16 [00:06<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n",
      "Core Argument:\n",
      "- The academic paper introduces the Tree of Thoughts (ToT) framework for language model inference, which allows for deliberate decision-making and exploration over coherent units of text.\n",
      "\n",
      "Evidence:\n",
      "- ToT significantly enhances language models' problem-solving abilities on tasks that require planning or search, such as the Game of 24, Creative Writing, and Mini Crosswords.\n",
      "- ToT addresses the limitations of existing language models by enabling exploration of different continuations within a thought process and heuristic-guided search.\n",
      "- The paper discusses the components of ToT, including thought decomposition, thought generation, state evaluation, and search algorithms.\n",
      "- Experiments show that ToT outperforms other prompting methods, such as Input-Output (IO) and Chain-of-Thought (CoT), in terms of success rates and exploration efficiency.\n",
      "\n",
      "Conclusions:\n",
      "- ToT is a general method for problem-solving with language models and offers generality and modularity.\n",
      "- ToT improves the coherency and success rate of language models in solving various tasks.\n",
      "- The ToT framework combines associative and deliberative thinking processes, providing value estimation and self-reflection mechanisms for decision-making.\n",
      "- ToT can be seen as a modern rendition of classical search methods and has potential for future advancements in AI.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list of messages\n",
    "messages = []\n",
    "\n",
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can get the answer to the question.\n",
    "Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
    "Do not answer questions using your own knowledge, only use your functions.\n",
    "Begin!\"\"\"\n",
    "messages.append({\"role\": \"system\"\n",
    "                 ,\"content\": paper_system_message})\n",
    "\n",
    "# Add a user message\n",
    "messages.append({\"role\": \"user\"\n",
    "                 ,\"content\": \"Hi, can you examine tree of thought reasoning for LLMs?\"})\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    messages, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "print(assistant_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "132f7410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Getting search results\n",
      "Downloading \"Proximal Policy Optimization and its Dynamic Version for Sequence Generation\" to knowledge base\n",
      "Downloading \"Lifetime policy reuse and the importance of task capacity\" to knowledge base\n",
      "Downloading \"Neural PPO-Clip Attains Global Optimality: A Hinge Loss Perspective\" to knowledge base\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n",
      "Core Argument:\n",
      "- The paper discusses the use of Proximal Policy Optimization (PPO) in sequence generation tasks, specifically in the context of chit-chat chatbots.\n",
      "- The authors argue that PPO is a more efficient reinforcement learning algorithm compared to policy gradient, which is commonly used in these tasks.\n",
      "- They propose a dynamic approach for PPO (PPO-dynamic) and demonstrate its efficacy in synthetic experiments and chit-chat chatbot tasks.\n",
      "\n",
      "Evidence:\n",
      "- PPO-dynamic achieves a high precision score in a synthetic counting task, comparable to other algorithms such as REINFORCE and MIXER.\n",
      "- In the chit-chat chatbot task, PPO-dynamic achieves a slightly higher BLEU-2 score than REINFORCE and PPO.\n",
      "- The learning curves of PPO and PPO-dynamic are more stable than policy gradient, and PPO-dynamic converges faster.\n",
      "\n",
      "Conclusions:\n",
      "- PPO is a better optimization method for sequence learning compared to policy gradient.\n",
      "- PPO-dynamic further improves the optimization process by dynamically adjusting the hyperparameters.\n",
      "- PPO can be used as a new optimization method for GAN-based sequence learning for better performance.\n"
     ]
    }
   ],
   "source": [
    "# Add another user message\n",
    "messages.append({\"role\": \"user\"\n",
    "                 ,\"content\": \"How about PPO using sequence generation, can you explain that to me\"})\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    messages, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = updated_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "print(assistant_message['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0141bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function not required, responding to user\n",
      "I apologize for the confusion. I am unable to provide direct links. However, I can provide the top three reasons based on the paper \"Proximal Policy Optimization and its Dynamic Version for Sequence Generation\" by Wu et al.\n",
      "\n",
      "1. Efficiency: PPO has been proven to be a more efficient reinforcement learning algorithm compared to policy gradient. It achieves stable and faster convergence in sequence generation tasks, such as chit-chat chatbots, as demonstrated by experiments conducted in the paper.\n",
      "\n",
      "2. Stability: PPO exhibits more stable learning curves compared to policy gradient. It avoids the issues of high variance and sensitivity to hyperparameters that can be encountered with policy gradient methods, leading to more reliable and consistent training.\n",
      "\n",
      "3. Performance: PPO outperforms policy gradient in terms of both stability and overall performance. It achieves higher precision scores in tasks like synthetic counting and higher BLEU-2 scores in chit-chat chatbot tasks. These results highlight the effectiveness and superiority of PPO in sequence generation.\n",
      "\n",
      "You can find more details in the paper \"Proximal Policy Optimization and its Dynamic Version for Sequence Generation\" by Wu et al.\n"
     ]
    }
   ],
   "source": [
    "# Add another user message\n",
    "messages.append({\"role\": \"user\"\n",
    "                 ,\"content\": \"What are the top three reasons PPO is better than policy gradient? Provide a reference to the paper you get your answer from so I can verify it.\"})\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    messages, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = updated_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "print(assistant_message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fa7ae",
   "metadata": {},
   "source": [
    "If the LLM is inconsistent in calling the functions or supplying the arguments you want, you should dedicate time to engineering both the function descriptions and the system prompt as these have a huge bearing on the output. If you are using multiple functions, you should also take care to ensure the case for using each is clear and that there is not significant overlap between them - if this happens, you will have an uneven user experience where the LLM sometimes calls one function and sometimes calls another.\n",
    "\n",
    "For production solutions you should also consider using a more permanent storage and retrieval solution such as a vector database to minimize cost and latency as you scale. There are details on the sorts of solutions available on our [documentation](https://platform.openai.com/docs/guides/embeddings/how-can-i-retrieve-k-nearest-embedding-vectors-quickly) and in the [cookbook](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases).\n",
    "\n",
    "We hope you've enjoyed this introduction to function calling and long document summarization, and we look forward to seeing what you build!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kl_fc",
   "language": "python",
   "name": "kl_fc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
