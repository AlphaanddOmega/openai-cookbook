{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3af9e12",
   "metadata": {},
   "source": [
    "# How to use OpenAI function calling to query APIs\n",
    "\n",
    "This tutorial takes you through an example of using OpenAI's new function calling feature to call an external API, such as weather forecasts to NBA statistics. Upon completion, you will be prepared to take on more production-ready scenarios of deploying functions with your own knowledge bases, databases, and other services.\n",
    "\n",
    "### Getting started\n",
    "\n",
    "Some basic knowledge of Python and GitHub is helpful for this tutorial. Before diving in, make sure to set up an OpenAI API key and walk through the quickstart tutorial. This will give a good intuition on how to use the API to its full potential.\n",
    "\n",
    "Python is used as the main programming language along with the OpenAI API. If you run into any issues working through this tutorial, please ask a question on the OpenAI Community Forum.\n",
    "\n",
    "To start with the code, clone the full code for this tutorial on GitHub. Alternatively, follow along and copy each section into a Jupyter notebook and run the code step by step, or just read along. A good way to avoid any issues is to set up a new virtual environment and install the required packages by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7d9a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0917c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8885f0d",
   "metadata": {},
   "source": [
    "## Defining functions\n",
    "\n",
    "An OpenAI ```function``` is defined with a ```name``` that the model will identify it with, a ```description``` that describes when to use it, and ```parameters``` that define what information the LLM needs to gather to use the function. The ```parameters``` can be required or optional, and you can also force the LLM to call a function (or not to call one) should your application logic demand it - for more details on these please refer to [this cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb) and our function calling [docs](https://platform.openai.com/docs/guides/gpt/function-calling).\n",
    "\n",
    "The LLM should then check the user's message each time to see whether a ```function``` is necessary, and if it does it will return a response that includes the ```finish_reason``` of ```function_call```, triggering your application to call the named function. \n",
    "\n",
    "For our example, we'll create two functions, ```get_current_weather``` and ```get_n_day_weather_forecast```. This will not connect with a real API, but will illustrate how functions work and how you can control them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "064b27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_n_day_weather_forecast\",\n",
    "        \"description\": \"Get an N-day weather forecast\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "                \"num_days\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The number of days to forecast\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\", \"num_days\"]\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fa950",
   "metadata": {},
   "source": [
    "## Calling the OpenAI API with functions\n",
    "\n",
    "We'll begin by asking it for the weather and seeing how the LLM chooses to react. We expect it to identify that we want to call one of the functions, and prompting us for any additional info it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6669db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x16a410350> JSON: {\n",
       "  \"content\": \"Sure, I can help you with that. Could you please provide me with your location so that I can fetch the current weather for you?\",\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5457e16",
   "metadata": {},
   "source": [
    "We'll clarify our location so it can continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3787bea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x16a4ddd30> JSON: {\n",
       "  \"finish_reason\": \"function_call\",\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"content\": null,\n",
       "    \"function_call\": {\n",
       "      \"arguments\": \"{\\n  \\\"location\\\": \\\"Glasgow, Scotland\\\",\\n  \\\"format\\\": \\\"celsius\\\"\\n}\",\n",
       "      \"name\": \"get_current_weather\"\n",
       "    },\n",
       "    \"role\": \"assistant\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"I'm in Glasgow, Scotland.\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3dbf92",
   "metadata": {},
   "source": [
    "Great, we've now got a completed function_call. We can tell this by the ```finish_reason``` which is set to ```function_call```. The ```name``` of the function to be called and the ```arguments``` to be passed in to that function are in the ```function_call``` object to be extracted.\n",
    "\n",
    "We'll now prompt it with a number of days mentioned to try to get it to the target our second function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "188b416d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x16a410410> JSON: {\n",
       "  \"content\": \"Sure! Please provide me with the number of days you would like to know the weather forecast for.\",\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in Glasgow, Scotland over the next x days\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25932a",
   "metadata": {},
   "source": [
    "It should have prompted us for the days, which we'll supply to complete the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9941562f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x16a410710> JSON: {\n",
       "  \"finish_reason\": \"function_call\",\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"content\": null,\n",
       "    \"function_call\": {\n",
       "      \"arguments\": \"{\\n  \\\"location\\\": \\\"Glasgow, Scotland\\\",\\n  \\\"format\\\": \\\"celsius\\\",\\n  \\\"num_days\\\": 5\\n}\",\n",
       "      \"name\": \"get_n_day_weather_forecast\"\n",
       "    },\n",
       "    \"role\": \"assistant\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\"role\": \"user\", \"content\": \"5 days\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=functions\n",
    ")\n",
    "chat_response[\"choices\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9547c22",
   "metadata": {},
   "source": [
    "Another completed function call - we've now successfully prompted our LLM to call both of the functions we provided. However, this required careful input by the user - in some cases there will be clear cases that we'll want to call a function deterministically, or bar the LLM from calling functions.\n",
    "\n",
    "For both of these cases, we use the ```function_call``` parameter for ```ChatCompletion``` to get the outcome we want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4b3c9",
   "metadata": {},
   "source": [
    "## Forcing the use of specific functions or no function\n",
    "\n",
    "We can force the model to use a specific function, for example ```get_n_day_weather_forecast``` by using the ```function_call``` argument. By doing so, we force the model to make assumptions about how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "233d6956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x16a4ddf10> JSON: {\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"arguments\": \"{\\n  \\\"location\\\": \\\"Toronto, Canada\\\",\\n  \\\"format\\\": \\\"celsius\\\",\\n  \\\"num_days\\\": 1\\n}\",\n",
       "    \"name\": \"get_n_day_weather_forecast\"\n",
       "  },\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in this cell we force the model to use get_n_day_weather_forecast\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=functions,function_call={\"name\": \"get_n_day_weather_forecast\"}\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8881fc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x16a4ddbb0> JSON: {\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"arguments\": \"{\\n  \\\"location\\\": \\\"Toronto, Canada\\\",\\n  \\\"format\\\": \\\"celsius\\\"\\n}\",\n",
       "    \"name\": \"get_current_weather\"\n",
       "  },\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we don't force it, it may call the other function\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8d53e7",
   "metadata": {},
   "source": [
    "We can also force the model to not use a function at all. By doing so we prevent it from producing a proper function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5cba27f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x1089b31d0> JSON: {\n",
       "  \"content\": \"{\\n  \\\"location\\\": \\\"Toronto, Canada\\\",\\n  \\\"format\\\": \\\"celsius\\\"\\n}\",\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=functions,function_call='none',\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6908c4",
   "metadata": {},
   "source": [
    "## Calling an external function\n",
    "\n",
    "In most cases when using function calling you'll want to take the ```arguments``` and plug them into an API to get some result. We'll add in an external API to test this flow out end-to-end. It will work like this:\n",
    "- Our user will ask the LLM a question.\n",
    "- The LLM will prompt the user for the required information.\n",
    "- Once the LLM has what it needs, it will output a function call.\n",
    "- We'll take the function call and use it to call an API to get some results.\n",
    "- We'll hand those results back to the LLM, who will use them to answer the user.\n",
    "\n",
    "Lets test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c422f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_functions = [\n",
    "    {\n",
    "        \"name\": \"get_stats\",\n",
    "        \"description\": \"Get NBA stats for a player or team.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Name of the team or player that we want to get stats for.\",\n",
    "                },\n",
    "                \"type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"player\", \"team\"],\n",
    "                    \"description\": \"Whether the stats are for a player or team. Infer this from the user's question.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"name\", \"type\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fba2fe",
   "metadata": {},
   "source": [
    "We'll add a simple function to print the conversation we create with the various roles highlighted so we can audit the back-and-forth between ```user``` and ```assistant```, supported by the ```system``` message to give instruction and the ```function``` results which give response for the LLM to act on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d312d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    formatted_messages = []\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            formatted_messages.append(f\"system: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            formatted_messages.append(f\"user: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            formatted_messages.append(f\"assistant: {message['function_call']}\\n\")\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            formatted_messages.append(f\"assistant: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            formatted_messages.append(f\"function ({message['name']}): {message['content']}\\n\")\n",
    "    for formatted_message in formatted_messages:\n",
    "        print(\n",
    "            colored(\n",
    "                formatted_message,\n",
    "                role_to_color[messages[formatted_messages.index(formatted_message)][\"role\"]],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "53087084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.static import players, teams\n",
    "from nba_api.stats.endpoints import commonplayerinfo, teamplayerdashboard\n",
    "\n",
    "\n",
    "def get_player_stats(player_name):\n",
    "    \n",
    "    # Find players by first name.\n",
    "    player_id = players.find_players_by_full_name('durant')[0]['id']\n",
    "    \n",
    "    # Get common player info for player\n",
    "    player_info = commonplayerinfo.CommonPlayerInfo(player_id=player_id)\n",
    "    \n",
    "    player_df = player_info.player_headline_stats.get_data_frame()\n",
    "    \n",
    "    return player_df\n",
    "\n",
    "def get_team_stats(team_name):\n",
    "    \n",
    "    # Find team by name.\n",
    "    team_id = teams.find_teams_by_full_name('cav')[0]['id']\n",
    "    \n",
    "    # Get info for the team\n",
    "    team_info = teamplayerdashboard.TeamPlayerDashboard(team_id=team_id)\n",
    "    \n",
    "    team_df = team_info.team_overall.get_data_frame()\n",
    "    \n",
    "    return team_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b30c1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function_call(message):\n",
    "    if message[\"function_call\"][\"name\"] == \"get_stats\":\n",
    "        \n",
    "        if json.loads(message[\"function_call\"][\"arguments\"])[\"type\"] == 'player':\n",
    "            player_name = json.loads(message[\"function_call\"][\"arguments\"])[\"name\"]\n",
    "            results = get_player_stats(player_name)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            team_name = json.loads(message[\"function_call\"][\"arguments\"])[\"name\"]\n",
    "            results = get_team_stats(team_name)\n",
    "    else:\n",
    "        results = f\"Error: function {message['function_call']['name']} does not exist\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de853279",
   "metadata": {},
   "source": [
    "We'll now initiate a fresh conversation and ask some NBA related questions, checking that the LLM reacts as expected and fetches us the information we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d617af4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making function call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kevin Durant has an average of 29.1 points and 6.7 rebounds per game.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Answer user questions with the stats from your functions. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Give me the points and rebounds for Kevin Durant\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=nba_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "if chat_response['choices'][0]['finish_reason'] == 'function_call':\n",
    "    print('Making function call')\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "    function_response = execute_function_call(assistant_message)\n",
    "    messages.append({\"role\": \"function\", \"name\": assistant_message[\"function_call\"][\"name\"], \"content\": str(function_response)})\n",
    "    chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=nba_functions,function_call='none'\n",
    "    )\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "else:\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ddf378b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: Answer user questions with the stats from your functions. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
      "\u001b[0m\n",
      "\u001b[32muser: Give me the points and rebounds for Kevin Durant\n",
      "\u001b[0m\n",
      "\u001b[34massistant: {\n",
      "  \"arguments\": \"{\\n  \\\"name\\\": \\\"Kevin Durant\\\",\\n  \\\"type\\\": \\\"player\\\"\\n}\",\n",
      "  \"name\": \"get_stats\"\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[35mfunction (get_stats):    PLAYER_ID   PLAYER_NAME TimeFrame   PTS  AST  REB    PIE\n",
      "0     201142  Kevin Durant   2022-23  29.1  5.0  6.7  0.185\n",
      "\u001b[0m\n",
      "\u001b[34massistant: Kevin Durant has an average of 29.1 points and 6.7 rebounds per game.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print_conversation(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "40979311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making function call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Cleveland Cavaliers had a record of 51 wins and 31 losses last season, with a winning percentage of 0.622. They played a total of 82 games. The team's overall performance was ranked first in several statistical categories such as rebounds, assists, turnovers, steals, blocks, blocks against, personal fouls, free throw attempts, and points scored.\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Answer user questions with the stats from your functions. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"How did the Cavaliers do last season\"})\n",
    "chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=nba_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "if chat_response['choices'][0]['finish_reason'] == 'function_call':\n",
    "    print('Making function call')\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "    function_response = execute_function_call(assistant_message)\n",
    "    messages.append({\"role\": \"function\", \"name\": assistant_message[\"function_call\"][\"name\"], \"content\": str(function_response)})\n",
    "    chat_response = openai.ChatCompletion.create(\n",
    "    model=GPT_MODEL,messages=messages, functions=nba_functions,function_call='none'\n",
    "    )\n",
    "    #print(chat_response)\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "else:\n",
    "    assistant_message = chat_response[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "05e2f624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: Answer user questions with the stats from your functions. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\n",
      "\u001b[0m\n",
      "\u001b[32muser: How did the Cavaliers do last season\n",
      "\u001b[0m\n",
      "\u001b[34massistant: {\n",
      "  \"arguments\": \"{\\n  \\\"name\\\": \\\"Cavaliers\\\",\\n  \\\"type\\\": \\\"team\\\"\\n}\",\n",
      "  \"name\": \"get_stats\"\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[35mfunction (get_stats):   GROUP_SET     TEAM_ID            TEAM_NAME GROUP_VALUE  GP   W   L  W_PCT   \n",
      "0   Overall  1610612739  Cleveland Cavaliers     2022-23  82  51  31  0.622  \\\n",
      "\n",
      "      MIN   FGM  ...  REB_RANK  AST_RANK  TOV_RANK  STL_RANK  BLK_RANK   \n",
      "0  3976.0  3408  ...         1         1         1         1         1  \\\n",
      "\n",
      "   BLKA_RANK  PF_RANK  PFD_RANK  PTS_RANK  PLUS_MINUS_RANK  \n",
      "0          1        1         1         1                1  \n",
      "\n",
      "[1 rows x 56 columns]\n",
      "\u001b[0m\n",
      "\u001b[34massistant: The Cleveland Cavaliers had a record of 51 wins and 31 losses last season, with a winning percentage of 0.622. They played a total of 82 games. The team's overall performance was ranked first in several statistical categories such as rebounds, assists, turnovers, steals, blocks, blocks against, personal fouls, free throw attempts, and points scored.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pretty_print_conversation(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fa7ae",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If the LLM is inconsistent in calling the functions or supplying the arguments you want, you should dedicate time to engineering both the function descriptions and the system prompt as these have a huge bearing on the output. If you are using multiple functions, you should also take care to ensure the case for using each is clear and that there is not significant overlap between them - if this happens, you will have an uneven user experience where the LLM sometimes calls one function and sometimes calls another.\n",
    "\n",
    "For production solutions you should also consider using a more permanent storage and retrieval solution such as a vector database to minimize cost and latency as you scale. There are details on the sorts of solutions available on our [documentation](https://platform.openai.com/docs/guides/embeddings/how-can-i-retrieve-k-nearest-embedding-vectors-quickly) and in the [cookbook](https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases).\n",
    "\n",
    "We hope you've enjoyed this introduction to function calling and long document summarization, and we look forward to seeing what you build!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "func_calls",
   "language": "python",
   "name": "func_calls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
